{"cells":[{"cell_type":"markdown","metadata":{"id":"Fi0-b4FfOFDm","colab_type":"text","cell_id":"53922d75fae849acbfb0e8fc687ec8e0","deepnote_cell_type":"markdown"},"source":"## Pose Estimation with ICP","block_group":"0e6b24717c3f433ab24a0b0e7cf6cdf6"},{"cell_type":"code","metadata":{"id":"0pI7YvFePCOG","colab":{},"colab_type":"code","lines_to_end_of_cell_marker":2,"cell_id":"12c039d54abf4fb790813ea6a1491305","deepnote_cell_type":"code"},"source":"import numpy as np\nfrom pydrake.all import (\n    AddMultibodyPlantSceneGraph,\n    BaseField,\n    DiagramBuilder,\n    Fields,\n    MeshcatVisualizer,\n    Parser,\n    PointCloud,\n    Rgba,\n    RigidTransform,\n    RollPitchYaw,\n    RotationMatrix,\n    StartMeshcat,\n)\n\nfrom manipulation import running_as_notebook\nfrom manipulation.exercises.grader import Grader\nfrom manipulation.exercises.pose.test_pose_estimation import TestPoseEstimation\nfrom manipulation.icp import IterativeClosestPoint\nfrom manipulation.scenarios import AddMultibodyTriad\nfrom manipulation.station import AddPointClouds, MakeHardwareStation, load_scenario","block_group":"3c19efad0e4a4a32a4f13da4c303d99d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"73f3b2352b724f57bcb4bdb0c64d3de3","deepnote_cell_type":"code"},"source":"# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"ad9ddf5cc1b343faab61a618821fcdbb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mcSz0hOVtBd5","colab_type":"text","cell_id":"3692f148404643ebb8359d40cb8cd1d5","deepnote_cell_type":"markdown"},"source":"## Problem Description\nLast lecture, we designed pick and place trajectories **assuming** that the object pose ${}^W X^O$ was known. With all the tools we have learned for goemetric perception, it is time to relax this assumption and finally do pose estimation from sensor data.\n\nThe goal of the exercise is to give you some real-world experience into what dealing with depth cameras, and what it takes to go from a real depth image to the clean ICP formulation we learned.\n\n**These are the main steps of the exercise:**\n1. Perform Segmentation on the raw pointcloud of the scene to extract pointcloud from the object.\n2. Tune an off-the-shelf ICP solver and estimate the pose of the object.","block_group":"b9b1401a95e94943b41cc0883d493a8f"},{"cell_type":"markdown","metadata":{"id":"NowURj8YP9qb","colab_type":"text","cell_id":"e65189111eea4176a7b3a4392eca6f66","deepnote_cell_type":"markdown"},"source":"Before jumping into the main exercise, how should we computationally represent a pointcloud? If we say that pointcloud has $N$ points, then each point has a position in 3D, ${}^Cp^i$, as well as an associated color. Throughout this exercise, we will tend to store them as separate arrays of:\n- `3xN` numpy array where each row stores the XYZ position of the point in meters.\n- `3xN` numpy array where each row stores the RGB information of the point in `uint8` format.\n\nUnfortunately, numpy prefers a rowwise representation, so you might find yourself using the `.T` transpose operator to make numpy operations more natural/efficient.","block_group":"127843f06327472f8892103ef53eee7e"},{"cell_type":"code","metadata":{"cell_id":"05e6a7163bb54fafb0bb75c4944d8d82","deepnote_cell_type":"code"},"source":"def ToPointCloud(xyzs, rgbs=None):\n    if rgbs:\n        cloud = PointCloud(xyzs.shape[1], Fields(BaseField.kXYZs | BaseField.kRGBs))\n        cloud.mutable_rgbs()[:] = rgbs\n    else:\n        cloud = PointCloud(xyzs.shape[1])\n    cloud.mutable_xyzs()[:] = xyzs\n    return cloud","block_group":"53c820376ecc4acda443544263342294","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e9fQyeITSQkD","colab_type":"text","cell_id":"a1674f83d21e43a7bc0a1b9c2ccf772a","deepnote_cell_type":"markdown"},"source":"## Getting a Pointcloud of the Model ##\n\nBefore taking a pointcloud of the **scene**, we will need a pointcloud of the **model** to compare against. Generally, this can be done by using existing tools that convert 3D representations (meshes, signed distance functions, etc.) into pointclouds.\n\nSince our red foam brick is of rectangular shape, we'll cheat a bit and generate the points procedurally. When you click the cell below, you should be able to see the red brick and our pointcloud representation of the brick as blue dots.\n\nWe will save the model pointcloud in the variable `model_pcl_np`.","block_group":"540543b76e9c42c6942fb821196aaf35"},{"cell_type":"code","metadata":{"id":"kfyNbppxqGhr","colab":{},"colab_type":"code","cell_id":"373b89f0203e48d8acf2e0b8770881b3","deepnote_cell_type":"code"},"source":"def visualize_red_foam_brick():\n    \"\"\"\n    Visualize red foam brick in Meshcat.\n    \"\"\"\n    builder = DiagramBuilder()\n    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.0)\n    parser = Parser(plant)\n    parser.AddModelsFromUrl(\n        \"package://drake/examples/manipulation_station/models/061_foam_brick.sdf\"\n    )\n    AddMultibodyTriad(plant.GetFrameByName(\"base_link\"), scene_graph)\n    plant.Finalize()\n\n    # Setup Meshcat\n    MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat)\n    diagram = builder.Build()\n    context = diagram.CreateDefaultContext()\n    diagram.ForcedPublish(context)\n\n\ndef generate_model_pointcloud(xrange, yrange, zrange, res):\n    \"\"\"\n    Procedurally generate pointcloud of a rectangle for each side.\n    \"\"\"\n    # Decide on how many samples\n    x_lst = np.linspace(xrange[0], xrange[1], int((xrange[1] - xrange[0]) / res))\n    y_lst = np.linspace(yrange[0], yrange[1], int((yrange[1] - yrange[0]) / res))\n    z_lst = np.linspace(zrange[0], zrange[1], int((zrange[1] - zrange[0]) / res))\n\n    pcl_lst = []\n    # Do XY Plane\n    for x in x_lst:\n        for y in y_lst:\n            pcl_lst.append([x, y, zrange[0]])\n            pcl_lst.append([x, y, zrange[1]])\n\n    # Do YZ Plane\n    for y in y_lst:\n        for z in z_lst:\n            pcl_lst.append([xrange[0], y, z])\n            pcl_lst.append([xrange[1], y, z])\n\n    # Do XZ Plane\n    for x in x_lst:\n        for z in z_lst:\n            pcl_lst.append([x, yrange[0], z])\n            pcl_lst.append([x, yrange[1], z])\n\n    return np.array(pcl_lst).T\n\n\nvisualize_red_foam_brick()\nmodel_pcl_np = generate_model_pointcloud(\n    [-0.0375, 0.0375], [-0.025, 0.025], [0.0, 0.05], 0.002\n)\nmeshcat.SetObject(\"pcl_model\", ToPointCloud(model_pcl_np), rgba=Rgba(0, 0, 1, 1))","block_group":"c552a5ae9e6e46e1971bc4cd4c0b2713","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6yaPL78TUhD","colab_type":"text","cell_id":"3215c913ebd94b2087ce23946bf831a8","deepnote_cell_type":"markdown"},"source":"## Getting the Scene Pointcloud\n\nNow let's set up the ClutteringStation from last lecture and actually take a pointcloud snapshot of the scene with the `red_foam_brick`. We'll place the camera where we have good coverage of the bin. We'll also take a pointcloud snapshot without the `red_foam_brick` so that we can use it for segmentation later.\n\nNOTE: There are around `3e7` points that are trying to be published to the visualizer, so things might load slowly, and occasionally the Colab session might crash. Keep calm and run the cells from the beginning!","block_group":"201cc09f68724ad09b55fee839bac5ef"},{"cell_type":"code","metadata":{"id":"lA_9j_2pSsTY","colab":{},"colab_type":"code","cell_id":"e8b0789212dc493fbf4f104e25411ec8","deepnote_cell_type":"code"},"source":"meshcat.Delete()\n\n\ndef setup_clutter_station(with_brick=True):\n    builder = DiagramBuilder()\n\n    scenario_data = \"\"\"\ndirectives:\n- add_model:\n    name: bin0\n    file: package://manipulation/hydro/bin.sdf\n\n- add_weld:\n    parent: world\n    child: bin0::bin_base\n    X_PC:\n      rotation: !Rpy { deg: [0.0, 0.0, 90.0 ]}\n      translation: [-0.145, -0.63, 0.075]\n\n- add_model:\n    name: bin1\n    file: package://manipulation/hydro/bin.sdf\n\n- add_weld:\n    parent: world\n    child: bin1::bin_base\n    X_PC:\n      rotation: !Rpy { deg: [0.0, 0.0, 180.0 ]}\n      translation: [0.5, -0.1, 0.075]\n\"\"\"\n    if with_brick:\n        scenario_data += \"\"\"\n- add_model:\n    name: brick\n    file: package://manipulation/hydro/061_foam_brick.sdf\n    default_free_body_pose:\n        base_link:\n            translation: [-0.1, -0.6, 0.09]\n            rotation: !Rpy { deg: [0, 0, 18] }    \n\"\"\"\n    scenario_data += \"\"\"\n- add_model:\n    name: camera\n    file: package://manipulation/camera_box.sdf\n- add_weld:\n    parent: world\n    child: camera::base\n    X_PC:\n        translation: [-0.1, -0.8, 0.5]\n        rotation: !Rpy { deg: [-150, 0, 0] }\ncameras:\n    main_camera:\n        name: camera0\n        depth: True\n        X_PB:\n            base_frame: camera::base\n\"\"\"\n\n    scenario = load_scenario(data=scenario_data)\n    station = builder.AddSystem(MakeHardwareStation(scenario, meshcat))\n    plant = station.GetSubsystemByName(\"plant\")\n    scene_graph = station.GetSubsystemByName(\"scene_graph\")\n    AddMultibodyTriad(\n        plant.GetFrameByName(\"base\", plant.GetModelInstanceByName(\"camera\")),\n        scene_graph,\n    )\n\n    # Send the point cloud to meshcat for visualization, too.\n    to_point_cloud = AddPointClouds(\n        scenario=scenario, station=station, builder=builder, meshcat=meshcat\n    )\n    if isinstance(to_point_cloud, list):\n        # TODO(russt): Remove this after Fall 2023 pset 4 is safely wrapped up\n        builder.ExportOutput(to_point_cloud[0].get_output_port(), \"camera_point_cloud\")\n    else:\n        builder.ExportOutput(\n            to_point_cloud[\"camera0\"].get_output_port(), \"camera_point_cloud\"\n        )\n\n    diagram = builder.Build()\n    diagram.set_name(\"clutter_station\")\n    return diagram\n\n\n# Take a pointcloud snapshot of the background to use for subtraction\ndiagram = setup_clutter_station(with_brick=False)\ncontext = diagram.CreateDefaultContext()\ndiagram.ForcedPublish(context)\n# Note: Use PointCloud here to make a copy of the data, since the diagram that\n# owns it will be garbage collected.\nscene_pcl_drake_background = PointCloud(\n    diagram.GetOutputPort(\"camera_point_cloud\").Eval(context)\n)\n\n# Take a pointcloud snapshot of the scene with the brick.\ndiagram = setup_clutter_station(with_brick=True)\ncontext = diagram.CreateDefaultContext()\ndiagram.ForcedPublish(context)\nscene_pcl_drake = diagram.GetOutputPort(\"camera_point_cloud\").Eval(context)\n\nplant = diagram.GetSubsystemByName(\"station\").GetSubsystemByName(\"plant\")\nplant_context = plant.GetMyContextFromRoot(context)\nX_WO = plant.EvalBodyPoseInWorld(plant_context, plant.GetBodyByName(\"base_link\"))","block_group":"1334cbae8b9f4c0a87bdf79f49887f1a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbgWqzgcUaAX","colab_type":"text","cell_id":"09ee22c1c60b48ee97603cfedd885a7b","deepnote_cell_type":"markdown"},"source":"## Visualizing the Problem ##\n\nThat was a lot of work, but if you run the below cell, Meshcat will finally show you a clean formulation of the main problem. We have 3 pointcloud objects in Meshcat:\n\n- `pcl_model`: Pointcloud of models\n- `pcl_scene`: Raw pointcloud of the foam-brick scene obtained from a RGBD camera.\n- `pcl_scene_background`: Raw pointcloud of the background obtained from a RGBD camera.\n\nIn case you forgot, In Meshcat's menu you can go into the `meshcat` tab and turn different objects on and off so that you can see what the background pointcloud looks like as well.\n\nNOTE: You might have to wait a bit until the bin pointcloud shows up.\n\n","block_group":"31ff7179bdb947b6aa848afad3ad0b44"},{"cell_type":"code","metadata":{"id":"0zMWmIMh5upv","colab":{},"colab_type":"code","cell_id":"94d7dac6295244dfb5024ea1d9b5de4d","deepnote_cell_type":"code"},"source":"meshcat.Delete()\n\nmeshcat.SetObject(\"pcl_model\", ToPointCloud(model_pcl_np), rgba=Rgba(0, 0, 1, 1))\nmeshcat.SetObject(\"pcl_scene\", scene_pcl_drake)\nmeshcat.SetObject(\"pcl_scene_background\", scene_pcl_drake_background)","block_group":"3268bbe445e346329e3717663b417ed2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TriejkVOWlsN","colab_type":"text","cell_id":"460448f8e4b54c46a637dd0f61a1b249","deepnote_cell_type":"markdown"},"source":"If we simply run ICP with `pcl_model` and `pcl_scene`, we might get a terrible result because there might be features in the background that the model is trying to run correspondence with. So we'd like to vet the problem a bit and perform **segmentation**: which parts of the scene pointcloud corresponds to an actual point on the `red_foam_brick`?\n\n\n**Now it's your turn to code!**\n\nBelow, you will implement a function `segment_scene_pcl` that takes in a pointcloud of the scene and return the relevant points that are actually on the `red_foam_brick`. But here are the rules of the game:\n- You **may** use color data, the background pointcloud, and any outlier detection algorithm that you can write to perform segmentation.\n- You may **not** explicitly impose conditions on the position to filter out the data. Remember that our goal is to estimate the pose in the first place, so using position will be considered cheating.\n- You may **not** use external libraries that are not in this notebook already.\n\nIn order to get full score for this assignment, you need to satisfy both criteria:\n- The number of false outliers (points which are not on the red brick but was caught by segmentation) must not exceed 80 points.\n- The number of missed inliers (points that are on the red brick but was not caught by segmentation) must not exceed 80 points.\n\nYou will be able to visualize your segmented pointclouds on Meshcat by running the cell.","block_group":"e94ab15fd6af409ea8002a141dd55382"},{"cell_type":"code","metadata":{"id":"2OY3fQjJGU3A","colab":{},"colab_type":"code","cell_id":"2973e42e6e5e467f9c6b4918d3fc0a53","deepnote_cell_type":"code"},"source":"def segment_scene_pcl(\n    scene_pcl_np,\n    scene_rgb_np,\n    scene_pcl_np_background,\n    scene_rgb_np_background,\n):\n    \"\"\"\n    Inputs:\n    scene_pcl_np: 3xN np.float32 array of pointclouds, each row containing xyz\n                    position of each point in meters.\n    scene_rgb_np: 3xN np.uint8   array of pointclouds, each row containing rgb\n                    color data of each point.\n    scene_pcl_np_background: 3xN np.float32 array of pointclouds, each row\n                    containing xyz position of each point in meters.\n    scene_rgb_np_background: 3xN np.uint8   array of pointclouds, each row\n                    containing rgb color data of each point.\n\n    Outputs:\n    scene_pcl_np_filtered: 3xM np.float32 array of pointclouds that are on the\n                    foam brick.\n    \"\"\"\n    ####################\n    # Fill your code here.\n\n    scene_pcl_np_filtered = scene_pcl_np\n    ####################\n\n    return scene_pcl_np_filtered\n\n\nscene_pcl_np_filtered = segment_scene_pcl(\n    scene_pcl_drake.xyzs(),\n    scene_pcl_drake.rgbs(),\n    scene_pcl_drake_background.xyzs(),\n    scene_pcl_drake_background.rgbs(),\n)\nmeshcat.SetObject(\n    \"pcl_scene_filtered\",\n    ToPointCloud(scene_pcl_np_filtered),\n    rgba=Rgba(0, 1, 0, 1),\n)","block_group":"4f725a20467c43928f5ef1f1c1d74d33","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FiZex_EDa-RC","colab_type":"text","cell_id":"a36fd4fe7d1f449c8ae5d42fa78a57b5","deepnote_cell_type":"markdown"},"source":"## ICP for Pose Estimation\n\nNow that we have a subset of scene points that we want to use to estimate the pose, let's do ICP to figure out what ${}^W X^O$ is. Instead of implementing your own ICP this time, we will use the version we developed in the chapter notes.\n\nWe know that ICP can't work very well without even a rough initialization. Let's assume that we at least know that the `red_foam_brick` is inside the bin, so that we can initialize the ${}^W X^O$ to be at the center of the bin with an identity rotation.","block_group":"b2afdad814f14f18b114c93556b0ace8"},{"cell_type":"code","metadata":{"id":"XADq9uCGK2rV","colab":{},"colab_type":"code","cell_id":"5baeb9e987e84f08a72fef4215e87d06","deepnote_cell_type":"code"},"source":"initial_guess = RigidTransform()\ninitial_guess.set_translation([-0.145, -0.63, 0.09])\ninitial_guess.set_rotation(RotationMatrix.MakeZRotation(np.pi / 2))","block_group":"1187d6f24a5b4477ad92985533b860ec","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cr4AkNyohLYF","colab_type":"text","cell_id":"24a1d85eca6a4a56833567ffb841a38d","deepnote_cell_type":"markdown"},"source":"Let's run the algorithm on your processed point cloud and see how we do!\n","block_group":"e8f7543b6aad4c1d9c4705cae3a6f8ac"},{"cell_type":"code","metadata":{"id":"n-wijftedyAH","colab":{},"colab_type":"code","cell_id":"3075764457cc4cfb8d6c6ae8718937ad","deepnote_cell_type":"code"},"source":"X_MS_hat, chat = IterativeClosestPoint(\n    p_Om=model_pcl_np,\n    p_Ws=scene_pcl_np_filtered,\n    X_Ohat=initial_guess,\n    meshcat=meshcat,\n    meshcat_scene_path=\"icp\",\n    max_iterations=25 if running_as_notebook else 2,\n)\nmeshcat.SetObject(\"pcl_estimated\", ToPointCloud(model_pcl_np), rgba=Rgba(1, 0, 1, 1))\nmeshcat.SetTransform(\"pcl_estimated\", X_MS_hat)\n\nnp.set_printoptions(precision=3, suppress=True)\nX_OOhat = X_MS_hat.inverse().multiply(X_WO)\n\nrpy = RollPitchYaw(X_OOhat.rotation()).vector()\nxyz = X_OOhat.translation()\n\nprint(\"RPY Error: \" + str(rpy))\nprint(\"XYZ Error: \" + str(xyz))","block_group":"7a47734ffdf949d89238cb70fd387635","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPmeRLtJk410","colab_type":"text","cell_id":"114b87fa03d040828ab138b214af9999","deepnote_cell_type":"markdown"},"source":"## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza.\n\nFor submission of this assignment, you must do as follows:.\n- Download and submit the notebook `pose_estimation_icp.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [4 pts] `segment_scene_pcl` correctly segments the scene by having less than 80 missed inliers and 80 false outliers.\n\nBelow is our autograder where you can check your score!","block_group":"a61100e9e558428a948f64047879d1cc"},{"cell_type":"code","metadata":{"id":"WrTSDUHk3S_J","colab":{},"colab_type":"code","cell_id":"4141eb01135749b19a7039fd93fea093","deepnote_cell_type":"code"},"source":"Grader.grade_output([TestPoseEstimation], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"66f504ce6f4849e9b6908739e7020baa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=202d1baa-334b-435d-ab65-4af187b480be' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.10.6 64-bit"},"language_info":{"name":"python","version":"3.11.5"},"deepnote_notebook_id":"4ab800bbb685492b9a2459bfd87d0dd4","deepnote_execution_queue":[]}}
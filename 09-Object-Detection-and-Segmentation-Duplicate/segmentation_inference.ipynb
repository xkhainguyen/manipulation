{"cells":[{"cell_type":"markdown","metadata":{"id":"DfPPQ6ztJhv4","cell_id":"3c1793ecb6034ab895b19b047f65f7d2","deepnote_cell_type":"markdown"},"source":"# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n","block_group":"2a58f64e0ddc43d79da4e33a4221ae6a"},{"cell_type":"code","metadata":{"id":"DBIoe_tHTQgV","source_hash":null,"execution_start":1704135896174,"execution_millis":3140,"deepnote_to_be_reexecuted":false,"cell_id":"b21a7f3c61244c0abc108163b0c08ed0","deepnote_cell_type":"code"},"source":"# Imports\nimport fnmatch\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]","block_group":"8b3cd86930174ab4a1841636adf64661","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwyE5A8DGtct","cell_id":"f605c0dd70ae4ffd9bbdd591feec86a3","deepnote_cell_type":"markdown"},"source":"# Download our bin-picking model\n\nAnd a small set of images for testing.","block_group":"e4c0f0b10f5943579469992dedab8155"},{"cell_type":"code","metadata":{"id":"_DgAgqauIET9","source_hash":null,"execution_start":1704135899328,"execution_millis":1511,"deepnote_to_be_reexecuted":false,"cell_id":"061df44148564273b7ce3852af6736ce","deepnote_cell_type":"code"},"source":"dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip .\n    !unzip -q clutter_maskrcnn_test.zip\n\nnum_images = len(fnmatch.filter(os.listdir(dataset_path), \"*.png\"))\n\n\ndef open_image(idx):\n    filename = os.path.join(dataset_path, f\"{idx:05d}.png\")\n    return Image.open(filename).convert(\"RGB\")\n\n\nmodel_file = \"clutter_maskrcnn_model.pt\"\nif not os.path.exists(model_file):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt .","block_group":"59e4cc573b914caa9e08548c75170856","execution_count":null,"outputs":[{"name":"stdout","text":"--2024-01-01 19:04:59--  https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip\nResolving groups.csail.mit.edu (groups.csail.mit.edu)... 128.30.2.44\nConnecting to groups.csail.mit.edu (groups.csail.mit.edu)|128.30.2.44|:443... connected.\nHTTP request sent, awaiting response... 403 Forbidden\n2024-01-01 19:04:59 ERROR 403: Forbidden.\n\n--2024-01-01 19:04:59--  http://./\nResolving . (.)... failed: No address associated with hostname.\nwget: unable to resolve host address ‘.’\nunzip:  cannot find or open clutter_maskrcnn_test.zip, clutter_maskrcnn_test.zip.zip or clutter_maskrcnn_test.zip.ZIP.\n","output_type":"stream"},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'clutter_maskrcnn_data'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip .\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip -q clutter_maskrcnn_test.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(fnmatch\u001b[38;5;241m.\u001b[39mfilter(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_image\u001b[39m(idx):\n\u001b[1;32m     10\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clutter_maskrcnn_data'"]}]},{"cell_type":"markdown","metadata":{"id":"xA8sBvuHNNH1","cell_id":"bdf3a85a57ec4a9e8fe8eb2834f45240","deepnote_cell_type":"markdown"},"source":"# Load the model","block_group":"b4178d1108a54f6bb8efd1e59c2aaad3"},{"cell_type":"code","metadata":{"id":"vUJXn15pGzRj","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"6c9eba7ead1749909a1e0e3a099868ed","deepnote_cell_type":"code"},"source":"import torchvision\nimport torchvision.transforms.functional as Tf\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model\n\n\nnum_classes = len(ycb) + 1\nmodel = get_instance_segmentation_model(num_classes)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.load_state_dict(torch.load(\"clutter_maskrcnn_model.pt\", map_location=device))\nmodel.eval()\n\nmodel.to(device)","block_group":"ac20e8b54e684345aa9d0d21149faefe","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6mYGFLxkO8F","cell_id":"f0e4c668164c478184fdfdf0b818535f","deepnote_cell_type":"markdown"},"source":"# Evaluate the network","block_group":"b9af5f22965d42c8a3b609f93238ae22"},{"cell_type":"code","metadata":{"id":"YHwIdxH76uPj","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"8445959036b04f5887bec95e2cf45471","deepnote_cell_type":"code"},"source":"# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9952)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])","block_group":"86c94df4c4834ee2a5d40bbf25e6e935","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmN602iKsuey","cell_id":"9173c5a69323443ab17a275b9815bd25","deepnote_cell_type":"markdown"},"source":"Printing the prediction shows that we have a list of dictionaries. Each element\nof the list corresponds to a different image; since we have a single image,\nthere is a single dictionary in the list. The dictionary contains the\npredictions for the image we passed. In this case, we can see that it contains\n`boxes`, `labels`, `masks` and `scores` as fields.","block_group":"87d6809e409941e294229d71bf95ee85"},{"cell_type":"code","metadata":{"id":"Lkmb3qUu6zw3","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"3cfe9e8bf93247c08ed381ca2ebc300d","deepnote_cell_type":"code"},"source":"prediction","block_group":"b37a50d4c18e4c049039354ee350ad2e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwT21rzotFbH","cell_id":"5132f2d138984133ae28bb8011563a1d","deepnote_cell_type":"markdown"},"source":"Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","block_group":"2332446cf73a43d293616d3abde4d8f9"},{"cell_type":"code","metadata":{"id":"bpqN9t1u7B2J","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"e9d8b8af25e34ff3905f0aff5f4ef514","deepnote_cell_type":"code"},"source":"img","block_group":"803a0a3a8fa149bcb5de618a11637397","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M58J3O9OtT1G","cell_id":"e3d3133a38804d7ba243aa7c0ec14daa","deepnote_cell_type":"markdown"},"source":"And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.","block_group":"3b8ff1dc5fdb4f8f857bdd6211026b0e"},{"cell_type":"code","metadata":{"id":"5v5S3bm07SO1","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"a383b7129d7d4602bfcbf0396ec0997b","deepnote_cell_type":"code"},"source":"N = prediction[0][\"masks\"].shape[0]\nfig, ax = plt.subplots(N, 1, figsize=(15, 15))\nfor n in range(prediction[0][\"masks\"].shape[0]):\n    ax[n].imshow(\n        np.asarray(\n            Image.fromarray(prediction[0][\"masks\"][n, 0].mul(255).byte().cpu().numpy())\n        )\n    )","block_group":"4793c598523d4185816c86d3491f10e8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9QAeX9HkDTx","cell_id":"58b8ca6bcdde4bf3a2c7da13c01f7dd3","deepnote_cell_type":"markdown"},"source":"# Plot the object detections","block_group":"f22a2197bf2848d0bd06718d702e46af"},{"cell_type":"code","metadata":{"id":"Z08keVFkvtPh","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"d914a038c8b543f4a0aed58670534bbb","deepnote_cell_type":"code"},"source":"import random\n\nimport matplotlib.patches as patches\n\n\ndef plot_prediction():\n    img_np = np.array(img)\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(img_np)\n\n    cmap = plt.get_cmap(\"tab20b\")\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    num_instances = prediction[0][\"boxes\"].shape[0]\n    bbox_colors = random.sample(colors, num_instances)\n    boxes = prediction[0][\"boxes\"].cpu().numpy()\n    labels = prediction[0][\"labels\"].cpu().numpy()\n\n    for i in range(num_instances):\n        color = bbox_colors[i]\n        bb = boxes[i, :]\n        bbox = patches.Rectangle(\n            (bb[0], bb[1]),\n            bb[2] - bb[0],\n            bb[3] - bb[1],\n            linewidth=2,\n            edgecolor=color,\n            facecolor=\"none\",\n        )\n        ax.add_patch(bbox)\n        plt.text(\n            bb[0],\n            bb[0],\n            s=ycb[labels[i]],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": color, \"pad\": 0},\n        )\n    plt.axis(\"off\")\n\n\nplot_prediction()","block_group":"bb84584de16d494abb05504df1884664","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIfmykN-t7XG","cell_id":"7fc6e92996fb498e85028d3e0495de14","deepnote_cell_type":"markdown"},"source":"# Visualize the region proposals \n\nLet's visualize some of the intermediate results of the networks.\n\nTODO: would be very cool to put a slider on this so that we could slide through ALL of the boxes.  But my matplotlib non-interactive backend makes it too tricky!","block_group":"bd867815d005475fb8dbdc1a2c2425e7"},{"cell_type":"code","metadata":{"id":"zBNqFb68td8N","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"43f7902e8d364e96b99e2e23a10aba5c","deepnote_cell_type":"code"},"source":"class Inspector:\n    \"\"\"A helper class from Kuni to be used for torch.nn.Module.register_forward_hook.\"\"\"\n\n    def __init__(self):\n        self.x = None\n\n    def hook(self, module, input, output):\n        self.x = output\n\n\ninspector = Inspector()\nmodel.rpn.register_forward_hook(inspector.hook)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nrpn_values = inspector.x\n\n\nimg_np = np.array(img)\nplt.figure()\nfig, ax = plt.subplots(1, figsize=(12, 9))\nax.imshow(img_np)\n\ncmap = plt.get_cmap(\"tab20b\")\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\nnum_to_draw = 20\nbbox_colors = random.sample(colors, num_to_draw)\nboxes = rpn_values[0][0].cpu().numpy()\nprint(f\"Region proposals (drawing first {num_to_draw} out of {boxes.shape[0]})\")\n\nfor i in range(num_to_draw):\n    color = bbox_colors[i]\n    bb = boxes[i, :]\n    bbox = patches.Rectangle(\n        (bb[0], bb[1]),\n        bb[2] - bb[0],\n        bb[3] - bb[1],\n        linewidth=2,\n        edgecolor=color,\n        facecolor=\"none\",\n    )\n    ax.add_patch(bbox)\nplt.axis(\"off\");","block_group":"f8428480263b4d9b90be58ec157c78db","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"f4a313365de04f23beeb560a9f8125b2","deepnote_cell_type":"markdown"},"source":"# Try a few more images","block_group":"769f29bfb3304fcda1a8d923e69a1a78"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"3732cdf59bc74c0e801bc00e644e6331","deepnote_cell_type":"code"},"source":"# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9985)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nplot_prediction()","block_group":"f8ae01d280984e53b6d7db5230f554f7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"2f06bc53e73541739d667612bf09f56e","deepnote_cell_type":"code"},"source":"","block_group":"8f275534f1c74e3fb0cad59d02ea8978","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cd3e7d17-0c32-40f9-b7c9-aa7a4e2fa280' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clutter_maskrcnn_inference.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.10.6 64-bit"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.8","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"9362ef99787847188dbee28bd5d11a4f","deepnote_persisted_session":{"createdAt":"2024-01-01T19:17:42.312Z"},"deepnote_execution_queue":[]}}